---
output: pdf_document
---

\doublespacing

\section{Introduction}
\label{sec2:Introduction}

A look into literary sources for OpRisk indicates [@acharyya2012current] that there is insufficient academic literature that looks to characterize its theoretical roots, as it is a relatively new discipline, choosing instead to focus on proposing a solution to the quantification of OpRisk. This chapter seeks to provide an overview of some of the antecedents of OpRisk measurement and management in the banking industry. As such, this chapter provides a discussion on why OpRisk is not trivial to quantify and attempts to understand its properties in the context of risk aversion with the thinking of practitioners and academics in this field.\medskip

According to @cruz2002modeling, FI's wish to measure the impact of operational events upon profit and loss (P\&L), these events depict the idea of explaining the *volatility of earnings* due to OpRisk data points which are directly observed and recorded. By seeking to incorporate new data intensive machine learning (ML) approaches to help understand the data, the framework analyses response variables that are decidedly non-normal, including categorical outcomes and discrete counts.\medskip

Due to commonly held beliefs [@aue2006lda], one of the main challenges toward the next generation of LDA models is in their incapability of dealing with the handling of statistical validation of qualitative adjustments, citing ill-conceived justification for its direct application to RC: However, in the advent of recent developments .viz ML techniquess, it is believed the advantages of conducting our investigations outweigh this disadvantage, shedding further light on our understanding of how forward-looking aspects of BEICF's affect firm-level OpRisk RC. Lastly, this resolves the problem associated with the context dependent nature of OpRisk as an apparent gap in the literature.

\section{The theoretical foundation of OpRisk}
\label{sec:The theoretical foundation of OpRisk}

@hemrit2012major argue that common and systematic operational errors in hypothetical situations poses presumtive evidence that OpRisk events, assuming that the subjects have no reason to disguise their preferences, are created sub-consciously. This study purports, supported by experimental evidence behavioural finance theories should take some of this behaviour into account when trying to explain in the context of a model, how investors maximise a specific utility/value function.\medskip

In a theoretical paper, @wiseman1997longitudinal discussed several organizational and behavioural theories, such as prospect theory (PT), which influence managerial risk-taking attitudes. Their findings demonstrate that behavioural views, such as PT and the behavioural theory of the firm explain risk seeking and risk averse behaviour in the context of OpRisk even after agency based influences are controlled for. Furthermore, they challenge arguments that behavioral influences are masking underlying root causes due to agency effects. Instead they argue for mixing behavioral models with agency based views obtaining more complete explanations of risk preferences and risk taking behavior [@wiseman1997longitudinal]. \medskip

@wiseman1997longitudinal suggest that managerial risk-taking attitudes are influenced by the decision (performance) context in which they are taken. In essence, managerial risk-taking attitude is considered as a proxy for measuring OpRisk  [@acharyya2012current]. In so doing, @wiseman1997longitudinal investigate more comprehensive economic theories viz.  PT and the behavioural theory of the firm, that prove relevant to complex organizations who present a more fitting measure for OpRisk. Upon further investigation, @barberis2003survey reveal that in finance, behavioral theory explains whether certain financial phenomena can be viewed as the result of less than fully rational thinking. Their argument goes, that through integrating OpRisk management into behavioral theory it may be possible to improve our understanding of firm level RC by refining the resulting OpRisk models to account for these behavioral traits. Thus implying that people's economic preferences described in the model have an economic incentive to improve the OpRisk RC measure. \medskip

Despite the reality that OpRisk does not lend itself to scientific analysis in the way that market risk and credit risk do, someone must do the analysis, value the RC measurement and hope the market reflects this. Besides, financial markets are not objectively scientific, a large percentage of successful people have been lucky in their forecasts, it is not an area which lends itself to scientific analysis.

\section{Overview of operational risk management}
\label{sec:Overview of operational risk management}

It is important to note how OpRisk manifests itself: The causes and sources of operational loss events as observed phenomena associated with operational errors and are wide ranging [@king2001operational]. By definition, the occurence of a loss event is due to P\&L volatitlity from a payment, settlement or a negative court ruling within the capital horizon over a time period (of usually one year) [@einemann2018operational].  As such, P\&L volatitlity is not only related to the way firms finance their business, but also in the way they \emph{operate}.\medskip 

In operating practice, one assumes that on observing or on following instructions we are consciously analysing and accurately executing our tasks based on the information available. However, the occurence of operational loss events indicates that there are sub-concious faults in information processing, which we are not consciously aware of but ultimately lead to P\&L losses. These operational loss events are almost always initiated at the dealing phase of the investment banking process; which more often than not implicates front office (FO) personnel who bear the brunt of responsibility for the loss e.g., during the trading process in cases where OpRisk events occur as a result of a mismatch between the trade booked (booking in trade feed) and the details agreed by the trader.\medskip

The middle office (MO) and back offices (BO) conduct OpRisk managements' task of building mathematical models to be used to predict OpRisk losses and ultimately determine capital adequacy required to absorb these losses. The implications from modelling can be used to better understand the broad view of the overall company's OpRisk exposure, through P\&L attribution carried out from deal origination to settlement. For instance, the results of the model can be used to better understand the interrelationships between risk factors and potential dependencies on various mitigation and management strategies [@acharyya2012current] e.g., human error is a potential risk factor resulting P\&L losses, whose negative impacts can be mitigated by an efficient trade amendment policy offsetting the outflow of P\&L with an equal and opposite inflow or cash injection.\medskip

Furthermore [@acharyya2012current], organizations may hold OpRisk due to external causes such as failure of third parties or vendors (either intentionally or unintentionally) in maintaining promises or contracts. The criticism in the literature is that no amount of capital is realistically reliable for the determination of RC as a buffer to OpRisk, particularly the effectiveness of the approach of capital adequacy from external events as there is effectively no control over them.\medskip

\section{The loss collection data exercise (LCDE)}
\label{sec:The loss collection data exercise (LCDE)}

In this study, a new dataset with unique feature characteristics is developed using the official loss data collection exercise (LDCE), as defined by @basel2011operational for internal data. The dataset in question is at the level of individual loss events which can therefore be modelled in a granular way, which facilitates the reflection of loss-generating mechanisms [@einemann2018operational]: It is therefore also fundamental as part of this study to know when they happened, and be able to identify the root causes of losses arising from which OpRisk loss events. Similarly to the afore-mentioned, this study intoduces an analogous mathematical framework for EBOR modeling, however the proposed OR framework is better suited with a higher probability to determine the amount of capital necessary to absorb operational losses as it is applicable to a larger number of OpRisk types.\medskip

The LCDE is carried out drawing statistics directly from the trade generation and settlement system, which consists of a tractable set of documented trade detail extracted at the most granular level, i.e. on a trade-by-trade basis (as per number of events (frequencies) and associated losses (severities)) and then aggregated daily. The development, calibration and validation of EBOR models is challenginf since new types of data and ahigher degree of expert involvement acroos the institution is required, providing a transparent quantitative framework for combining forward-looking point-in-time data and historical loss experience [@einemann2018operational]. The afore-mentioned LDCE is an improved reflection of the risk factors by singling out the value-adding processes associated with individual losses on a trade-by-trade level. The dataset is then split into proportions and trained, validated and tested.

\section{Loss Distribution Approach (LDA)}
\label{sec:Loss Distribution Approach (LDA)}

The Loss Distribution Approach (LDA) is an AMA method whose main objective is to provide realistic estimates to calculate VaR for OpRisk RC in the banking sector and it's business units based on loss distributions that accurately reflect the frequency and severity loss distributions of the underlying data. Having calculated separately the frequency and severity distributions, we need to combine them into one aggregate loss distribution that allows us to produce a value for the OpRisk VaR. \medskip

We begin by defining some concepts:
\begin{itemize}
\item In line with Basel II, and according to @frachot2001loss, we consider a matrix consisting of business lines $BL$ and (operational) event types $ET$. The bank estimates, for each business line/event type (BL/ET) cell, the probability functions of the single event impact and the event frequency for the next three months. More precisely, in each cell of the BL/ET matrix separate distributions for loss frequency and severity are modeled and aggregated to a loss distribution at the group level. The aggregated operational losses can be seen as a sum $L$ of a random variables $N_1,\ldots,N_m$ representing loss frequencies, i.e., the values $N_k \in \mathbb{N_{>0}}$. for each $k \in \{1,\ldots,m\}$ individual operational losses \begin{math} (X_{k1}, \ldots, X_{kN_k})\end{math} are samples of the severity distribution $X_k$ of loss type $k$. The loss variable $L_k$ in this cell is defined by

\singlespacing
\begin{equation}\label{Lossvar}
L_k = \sum_{l=1}^{N_k}X_{kl} 
\end{equation}
\doublespacing

The aggregate loss distribution at group level is defined by

\singlespacing
\begin{equation}\label{AggLossvar}
\mathbf{L} = \sum_{k=1}^m L_k 
\end{equation}
\doublespacing

\item Three month daily statistics are taken of the time series of internal processing errors (frequency data) and their associated severities and used in each cell of the BL/ET matrix. Frequency refers to the number of events that occur within the specified time period (daily buckets) $T$ and $T + \tau$ and severity refers to the P\&L impact resulting from the frequency of events. The time (1 day bucket) period is chosen in order to ensure that the number of data points is sufficient for statistical analysis.
\end{itemize}

\subsection{Computing the frequency distribution}
\label{ssec:Computing the frequency distribution}

\begin{itemize}
\item Let $\mathbf{N}_{ij}$ be variable in random selection, representing \textbf{the number of times of process risk event failures} between times $T$ \& $T +\tau$. Suppose subscript $i$ refers to the $BL$ which ranges from \begin{math} 1, \ldots, k \end{math} and subscript $j$ to $ET$ which ranges from \begin{math} 1, \ldots, l \end{math} (e.g., suppose $j=1$ for Internal Fraud). We have taken a random sample implying that the observations \begin{math} N_{ij}\end{math}, {where} \begin{math}{i,j}= (1,1)\,,\;\ldots, (k,1)\end{math} are independent and identically distributed (i.i.d). 

\item The random variable $N_{i1}$\footnote{$N_{ij}$ \, where subscript $j=1$ since we are only dealing with $1$ event type i.e. process risk} has distribution function\footnote{The term distribution function is monotonic increasing function of $n$ which tends to $0$ as \begin{math} n \longrightarrow -\infty\end{math}, and to $1$ as \begin{math} n \longrightarrow \infty \end{math}} The random variable has distribution function (d.f.) \begin{math}\mathbf{P}_{i1}(n/\theta_0)  \end{math}, where $\theta_0$ is an unknown parameter of the estimated distribution.  The unknown parameter $\theta_0$ may be a scalar or a vector quantity \begin{math}\mathbf{\theta_0}\end{math}, for example, The Poisson distribution depends on one parameter called $\lambda$ whereas the univariate normal distribution depends on two parameters, $\mu$ and $\sigma ^2$, the mean and variance.  These parameters are to be estimated in some way. We use the Maximum Likelihood Estimate (m.l.e) which is that value of $\theta$ that makes the observed data \lq\lq most probable\rq\rq or \lq\lq most likely\rq\rq.\medskip

\item The d.f. \begin{math}\mathbf{P}_{i1}(n/\theta_0)  \end{math}, is the probability that $N_{i1}$ takes a value less than or equal to $n$, where $n$ is a small sample from the entire population of observed frequencies, i.e.

\singlespacing
\begin{equation}\label{PDF}
\mathbf{P}_{ij}(n)=Pr \left(N_{ij}\leq n \right) \quad{i,j}= (1,1),\ldots, (k,1)
\end{equation}
\doublespacing

\item The probability density function (p.d.f) : A density function is a non--negative function $p(n)$ whose integral, extended over the entire $x$ axis, is equal to $1$ for a given continuous random variable $X$. i.e. it is the area under the probability density curve, of the discrete random variable $N_{i1}$ takes discrete values of $n$ with finite probabilities. In the discrete case the term for p.d.f. is the probability function (p.f.) also called the probability mass function, i.e. $N_{i1}$ is given by the probability that the variable takes the value $n$, i.e.

\singlespacing
\begin{equation}\label{eqn3}
p_{ij}(n)=Pr\left(N_{ij} = n\right), \quad{i,j}= (1,1),\ldots, (k,1) 
\end{equation} 
\doublespacing

\item The r.h.s of equation~(\ref{PDF}) is the summation of the r.h.s of equation~(\ref{eqn3}), we derive a relation for the \textbf{loss frequency distribution} in terms of the (p.f): 

\singlespacing
\begin{equation}\label{eqn4} 
\mathbf{P}_{ij}(n)=\sum_{k=1}^{n_k} p_{ij}(n) \quad{i,j}= (1,1),\ldots, (k,1)
\end{equation}
\doublespacing

\end{itemize}

\subsection{Computing the severity distribution}

\begin{itemize}
\item Suppose $X_{ij}$ is a random variable representing \textbf{the amount of one loss event} in a cell of the BL/ET matrix. Define next period's loss in each cell $(i,j)$, where $i$ is the number of business line cells, \begin{math} {{L}^{T+1}}_{i,j}\end{math}: Operational loss for loss type $j=1$ (process risk). One models the amount of the total operational loss of type $j$ at a given time $T$ \& $T + 1$, over the future (say 3 months), as:

\singlespacing
\begin{equation}\label{eqn5}
{L}^{T+1}=\sum_{i=1}^{k}{L}^{T+1}_{i1}=\sum_{i=1}^{2}\sum_{l=1}^{{{N}_{i1}}^{T+1}}{{X}^{l}}_{i1} \quad l=1,2,\ldots, N_{i1} 
\end{equation}
\doublespacing

\item Let $ N_1, N_2,...,N_m $ (where $m$ in the number of combinations in the BL/ET matrix) be random variables that represent the loss frequencies. It is usually assumed that the random variables $X_{i1}$ are independently distributed and independent of the number of events $N_{m}$. A fixed number of a particular loss type would be denoted by ${{X}^{1}}_{i1}$, i.e the random variable \begin{math}{{X}^{l}}_{i1}\end{math}, represents random samples of the severity distribution [@aue2006lda].\medskip

The \textbf{loss severity distribution} is denoted by \begin{math}\mathbf{F}_{i1}\end{math}. Since loss severity variate $X$ is continuous (i.e. can take on any real value), we define a level of precision $\emph{h}$ such that the probability of $X$ being within $\pm\emph{h}$ of a given number $x$ tends to zero. The loss severity, $X_{i1}$ has a (d.f.) \begin{math}\mathbf{F}_{i1}(x/\theta_1) \end{math}, where $\theta_1$ is an unknown parameter and $x$ is a small sample from the entire population of loss severity.

\item We define probability density in the continuous case as follows:

\singlespacing
\begin{eqnarray}
f_{X}(x) &=& \lim_{h\rightarrow 0}\frac{Pr[x < X \leq x + h]}{h}\nonumber\\
&=& \lim_{h\rightarrow 0}\frac{F_{X}(x + h) - F_{X}(x)}{h}\nonumber\\
&=&\frac{dF_{X}(x)}{dx} \label{eqn6a}
\end{eqnarray}
\doublespacing

operate with $\int\,dx$ on both sides of \ref{eqn6a}

\singlespacing
\begin{equation}
\mathbf{F}_{X_{ij}}(x)=\int_{k=1}^{\infty} f_{X_{ij}}(x)dx \quad{i,j}= (1,1),\ldots, (k,1)\label{eqn6b}
\end{equation}
\doublespacing

where $f_{X_{ij}}(x)$ is the probability density function (p.d.f.). Once again, the subscript $X$ identifies the random variable for severity (P\&L impact) of one loss event while the argument $x$ is an arbitrary sample of the severity events.
\end{itemize}

\subsection{Formal Results}

Having calculated both the frequency and severity process we need now to combine them in one aggregate loss distribution that allows us to predict an amount for  the operational losses to a degree of confidence. There is no simple way of aggregating the frequency and severity distribution. Numerical approximation techniques (computer algorithms) successfully bridge the divide between theory and implementation for the problems of mathematical analysis.\medskip


The aggregated losses at time $t$ are given by $\vartheta(t) = \sum_{n=1}^{N(t)} X_{n}$ (where X represents individual operational losses). Frequency and severity distributions are estimated, e.g., the poisson distribution is a representation of a discrete variable commonly used to model operational event frequency (counts), and a selection from continuous distributions which can be linear (e.g. gamma distribution) or non-linear (e.g. lognormal distribution) for operational loss severity amounts. The compound loss distribution $\mathbf{G}(t)$ can now be derived.  Taking the aggregated losses we obtain:

\singlespacing 
\begin{equation}\label{Compound_losses}
\mathbf{G}_{\vartheta(t)}(x)=Pr[\vartheta(t)\leq x]=Pr\left(\sum_{n=1}^{N(t)}X_{n} \leq x\right)
\end{equation}
\doublespacing

For most choices of $N(t)$ and $X_{n}$, the derivation of an explicit formula for $\mathbf{G}_{\vartheta(t)}(x)$ is, in most cases impossible. $\mathbf{G}(t)$ can only be obtained numerically using the Monte Carlo method, Panjer's recursive approach, and the inverse of the characteristic function [@frachot2001loss; @aue2006lda; @panjer2006operational; \& others]. \medskip

\begin{itemize}
\item We now introduce the aggregate loss variable at time $t$ given by $\vartheta(t)$. This new variable represents \textbf{the loss for business line $i$ and event type $j$}. The aggregate loss is defined by \begin{math} \vartheta(t) = \sum_{n=1}^{N(t)} X_{n} \end{math} (where X represents individual operational losses). Once frequency and severity distributions are estimated, the compound loss distribution \begin{math} \mathbf{G}(t)\end{math} can be derived.  Taking the aggregated losses we obtain:

\singlespacing
\begin{equation}\label{eqn6}
\mathbf{G}_{\vartheta(t)}(x)=Pr[\vartheta(t)\leq x]=Pr\left(\sum_{n=1}^{N(t)}X_{n} \leq x\right)
\end{equation}
\doublespacing

\item The derivation of an explicit formula for \begin{math}\mathbf{G}_{\vartheta(t)}(x) \end{math} is, in most cases impossible. Again we implicitly assume that the processes \{$N(t)$\} and $\{X_{n}\}$ are independent and identically distributed (i.i.d).  Deriving the analytical expression for \begin{math}\mathbf{G}_{\vartheta(t)}(x) \end{math}, we see a fundamental relation corroborated by @frachot2001loss, @cruz2002modeling, @embrechts2013modelling, \& others:

\singlespacing
\begin{equation}\label{eqn7}
\mathbf {G}_{\vartheta(t)}(x)=\left\{\begin{array}{rcl}
                 &\sum_{n,k=0,1}^{\infty} p_{k}(n)\mathbf{F}_{X}^{k\star}(x) &x>0\\ &p_{k}(0) &x=0
                 \end{array}\right\}
\end{equation}
\doublespacing

where $\star $ is the \emph{convolution} operator on d.f.'s, \begin{math}\mathbf{F}^{k\star} \end{math} is the k-fold convolution of \begin{math}\mathbf{F} \end{math} with itself. The convolution of two functions $f(x)$ and $g(x)$ is the function
\singlespacing
\begin{equation}
\int_{0}^{x}f(t)g(x-t)dt
\end{equation}
\doublespacing , i.e. \begin{math} \mathbf{F}_{X}^{k\star}(x)=Pr(X_1 + \ldots + X_k \leq x) \end{math}, the d.f. of the sum of $k$ independent random variables with the same distribution as $X$. 

\item The aggregate loss distribution \begin{math} \mathbf {G}_{\vartheta(t)}(x) \end{math} cannot be represented in analytic form, hence approximations, expansions, recursions of numerical algorithms are proposed to overcome this problem.  For purposes of our study, an approximation method will do. One such method consists of taking a set \begin{math} \langle \vartheta_1, \ldots , \vartheta_s \rangle \end{math}, otherwise known as the ideal generated by elements \begin{math} \vartheta_1, \ldots , \vartheta_s \end{math} which are $s$ simulated values of the random variable $\vartheta_{ij}$  for $s = 1,\ldots, S$ [@fraleigh2003first].\medskip

This method is popularly known as Monte Carlo simulation coined by physicists in the 1940's, it derives its name and afore--mentioned popularity to its similarities to games of chance. The way it works in layman's terms is; in place of simulating scenario's based on a base case, any possible scenario through the use of a probability distribution (not just a fixed value) is used to simulate a model many times. In the LDA separate distributions of frequency and severity are derived from loss data then combined by Monte Carlo simulation. 
\end{itemize}

\subsection{Dependence Effects (Copulae)}

The standard assumption in the LDA is that frequency and severity distributions in a cell are independent and the severity samples are i.i.d. According to Basel II, dependence effects in OpRisk are not considered. Economic capital allocation however, could benefit if it were determined in a way that recognises the risk-reducing impact of correlation effects between the risks of the BL/ET combinations. Concluding remarks from a study by @urbina2014application allude that failure to account for correlation may lead to risk management practices that are unfair, as evidenced in an example using data from the banking sector. \medskip

One of the main issues we are confronted with in OpRisk measurement is the aggregation of individual risks (in each BL/ET element). A powerful concept to aggregate the risks -- the \emph{copula} function -- has been introduced in finance by @embrechts2002correlation. Copulas have been used extensively in finance theory lately and are sometimes held accountable for recent global financial failures, e.g. the global credit crunch of 2008 - 2009. They are nevertheless still applicable and in use for OpRisk as operational risk models follow a different stochastic process to other areas of risk, e.g. operational VaR is subject to more jumps than market VaR and is thought to be discrete whereby market VaR is continuous. \medskip

Copulas are functions which conveniently incorporate correlation into a function that combines each of the frequency (marginal) distributions to produce a single bivariate cumulative distribution function. Our model is used to determine the aggregate (bivariate) distribution of a number of correlated random variables through the use a Clayton copula. Dependence matters due to the effect of the addition of risk measures over different risk classes (cells in the BL/ET matrix). \medskip

More precisely, the frequency distributions of the individual cells of the BL/ET matrix are correlated through a Clayton copula in order to replicate observed correlations in the observed data. Let $m$ be the number of cells, $\mathbf{G_1}, \mathbf{G_2},...,\mathbf{G_m}$ the distribution functions of the frequency distributions in the individual cells and $\mathbf{C}$ the so--called copula. Abe Sklar  proved in 1959 through his theorem (Sklar's  Theorem) that for any joint distribution $\mathbf{G}$ the copula $\mathbf{C}$ is unique.  $\mathbf{C}$ is a distribution function on $[0,1]^{m}$ with uniform marginals. We refer to a recent article by @chavez2006quantitative for further information: It is sufficient to note that $\mathbf{C}$ is unique if the marginal distributions are continuous. 

\singlespacing
\begin{equation}\label{eqn7a}
\mathbf{G}(x_1, \ldots, x_m) = \mathbf{C}\left(\mathbf{G_1}(x_1), \ldots, \mathbf{G_m}(x_m) \right)
\end{equation}
\doublespacing

Conversely, for any copula $\mathbf{C}$ and any distribution functions $\mathbf{G_1}, \mathbf{G_2},...,\mathbf{G_m}$, the functions $\mathbf{C}\left(\mathbf{G_1}(x_1), \ldots, \mathbf{G_m}(x_m) \right)$ is a joint distribution function with marginals $\mathbf{G_1}(x_1), \ldots, \mathbf{G_m}(x_m)$. Moreover, combining given marginals with a chosen copula through Equation \ref{eqn7a} always yields a multivariate distribution with those marginals. The copula function has then a great influence on the aggregation of risk.

\section{LDA model shortcomings}
\label{ssec:LDA model shortcomings}

After most complex banks adopted the LDA for accounting for RC, significant biases and delimitations in loss data remain when trying to attribute capital requirements to OpRisk losses [@frachot2001loss]. OpRisk is related to the internal processes of the FI, hence the quality and quantity of internal data are of greater concern as the available data could be rare and/or of poor quality. Such expositions are unsatisfactory if OpRisk, as @cruz2002modeling professes, represents the next frontier in reducing the riskiness associated with earnings. @de2015combining and @galloppo2014review sought to address the shortcomings of @frachot2001loss by finding possible ways to improve the problems of biases, such as "ommitted varaible bias" (OVB) and data delimitation in operational risk management. Furthermore, @opdyke2014estimating advanced on this problem through a study intending on eliminating biases due to heavy tailed distributions i.e., overestimation of capital adequacy estimates in a time lag after realised losses due to extrapolation to the 99.9th percentile and an overstretched distribution.\medskip

@de2015combining, @galloppo2014review, @opdyke2014estimating \& others follow along lines in the literature of recent attempts aimed at finding a statistical-based model for OpRisk capital calculation, which suggest integrating internal and external data as well as scenario assessments to endeavour on improving on accuracy for the estimates. In recent work, @badescu2015modeling reported that LDA modelling is found wanting due to the very complex characteristics of data sets required to establish OpVaR. Furthermore, insightful are continually emerging new found techniques are being built to deal with these issues that arise in LDA modeling; opening new and contentious areas of work, keeping practitioners and academics guessing at what revolutionary phase may follow w.r.t the latest research methods. \medskip

@opdyke2014estimating, @agostini2010combining, @de2015combining, @galloppo2014review, and others seem to explicate how greater accuracy, precision and robustness uphold a valid and reliable estimate for OpRisk capital as defined by Basel II/III. Transforming this basic knowledge into a \lq\lq risk culture\rq\rq\ or firm-wide knowledge for the effective management of OpRisk serves as a starting point forming a control function that provides attribution and accounting support within a framework, methodology and theory for understanding OpRisk. FI's are beginning to implement sophisticated risk management systems similar to those for market and credit risk, linking theories which govern how these risk types are controlled to theories that govern financial losses resulting from OpRisk events. \medskip

@agostini2010combining also argued that banks should adopt an integrated model by combining a forward-looking component (scenario analysis) to the historical OpVaR, reinforcing foremost discussions in today's literature by involving subject matter expert analysis of the case, through an integration model which is based on the idea of estimating the parameters of the historical and subjective distributions and then combining them using advanced credibility theory (ACT). The basis for the use of ACT is the idea that a better estimation of the OpRisk measure can be obtained by combining the two sources of information advocating for the combined use of both experiences.\medskip

@agostini2010combining seek to explain through a weight called the credibility, the amount of credence given to two components (historical and subjective) determined by statistical uncertainty of information sources, as opposed to the conventional weighted average approach chosen on the basis of qualitative judgements. @agostini2010combining proposed the integration method be deemed as self-contained and independent of any arbitrary choice in the weights of the historical or subjective components of the model, which serves as a more compelling representation of facts.

\subsection{Current operational risk measurement modeling framework}
\label{sec:Current operational risk measurement modeling framework}

Historical severity curves obtained from historical loss counts that are usually presented in conventional quantification techniques, such as in LDA modelling, have been widely considered to be the most reliable models when used in OpRisk loss estimation. However, they are not useful and have not been very successfull when used to predict future losses, particularly of measure of more "predeictive" OpRisk types who capturing forward-looking aspects of the BEICFs thereof. As stated in the industry position paper, see @ama2013ama, these are OpRisk types with defined risk exposure and identifiable risk drivers, which are then incorporated as explanatory variables in "alternative" models whose aim is to replace the afore-mentioned LDA modelling techniques by measures using event frequencies based on actual exposures and available risk factors, instead of hisorical loss counts in the capital adequacy prediction problem [@einemann2018operational].

\subsection{Benefits and Limitations}
\label{ssec:Benefits and Limitations}

The basic idea of the integration methodologies in Subsection \ref{sec:Current operational risk measurement modeling framework} is to estimate the parameters of the frequency and severity distributions based on the historical losses and correct them; via a statistical theory, to include information coming from the scenario analysis. These approaches are deemed to have significant advantages over conventional LDA methods proposing that an optimal mix of the two modeling components i.e., historical and subjective parts, could better predict OpVaR over traditional methods. Particularly in the work by @agostini2010combining, whose integration model represents a benchmark in OpRisk measurement by including a component in the AMA model that is not obtained by a direct average of historical and subjective VaR.\medskip

These methods has the advantage of being completely self contained and independent of any arbitrary choice or weighting of the historical or subjective components in the model made by the analyst. These components weights are derived objectively, through robust means based on statistical uncertainties of information sources rather than through risk managers choices based on qualitative motivations. However, they suffer from not explaining the prerequisite need for coherence between the historical and subjective distribution functions, required for the model to work; particularly when in a number of papers [@chau2014robust] it's proposed that using mixtures of (heavy tailed) distributions commonly used in the setting of OpRisk capital estimation cannot be avoided [@opdyke2014estimating].\medskip
 
\section{EBOR methodology for capturing forward-looking aspects of ORM}
\label{sec:EBOR methodology for capturing forward-looking aspects of ORM}

In a theoretical paper, @einemann2018operational construct a mathematical framework for an EBOR model to quantify OpRisk for a portfolio of pending litigations. Their work unearths an invaluable contribution to the literature, discussing a strategy on how to integrate EBOR and LDA models by building hybrid frameworks which facilitate the migration of OpRisk types from a *classical* to an exposure-based treatment through a quantitative framework, capturing forward looking aspects of BEICF's [@einemann2018operational], a key source of the OpRisk data.\medskip

As mentioned in their paper [@einemann2018operational], they were the first to lay the groundwork for future development of their technique across industry, and to establish a common language through a strategy for integrating EBOR models and LDA models. In the former EBOR model they incorporate "predictable" loss types e.g., they test their hypothesis on a portfolio of pending litigations, litigations being "predictable" as far as when given the event triggering the filing of the litigation case had already happened, and only the final outcome in court has to be modelled. In the latter LDA modelling case, they consider LDA components which cover risks that are well reflected through historical events.

\subsection{The general exposure-based operational risk (EBOR) concept}
\label{ssec:The general exposure-based operational risk (EBOR) concept}

The general theory for measuring and allocating risk capital is independent of specific risk types: It is the basis from which standard risk measures are founded. Risk capital calculated at the aggregate level forms the basis from which the allocation of risk capital to individual events is derived; in fact @aue2006lda shows why a consistent framework for measuring risk and firm performance hindges, in fact, on a uniform application of risk theory to capital adequacy calculation for market, credit and Oprisk. In particular, standard risk measures like value-at-risk (VaR) or expected shortfall (ESF) are based on the Monte Carlo simulation of the loss distribution which is a numerical representation of a simple closed form of the total event distribution function. More precisely,

\singlespacing
\begin{eqnarray}
\mbox{VaR}(\alpha) &=& \mbox{inf}\{z \in \mathbb{R}\| \mathbb{P}(Z \leq z) \geq \alpha \},\\
\mbox{ESF}(\alpha) &=& \mathbb{E}(Z\| \mbox{VaR}(\alpha))
\end{eqnarray}
\doublespacing

The allocation of risk capital to BL/ET combination risk cells is based on ESF contributions, which is numerically evaluated in the tail of the aggregate loss distribution if a sample list of $Z$ has been calculated. The tail focused allocation technique is particularly well suited to model risk capital allocations to individual exposures, due to the fact that that each sample $z$ of $Z$ can be reduced down in granularity of the $n$ loss events .i.e., $\sum_{j=1}^n z_j$, defined as its contribution to the tail of the aggregate loss distribution [@aue2006lda]:

\singlespacing
\begin{eqnarray}
\mbox{ESF}_i(\alpha) = \mathbb{Z}(Z_i\| > \mbox(VaR)(\alpha)).
\end{eqnarray}
\doublespacing

If the underlying portfolio is limited in granularity, risk capital is allocated to a small number of portfolio constituents whose risk management strategy precludes tail focused allocation techniques like ESF which are based on a high quantile features designed to highlight risk concentrations [@einemann2018operational]. Decidedly desiring alternative techniques which give more weight to the body of the underlying distributions.\medskip

EBOR modeling techniques are specifically designed to quantify specific aspects of OpRisk consisting of determining the aggregate event loss variable $\mathbf{Y}$ which may have rather concentrated risk profiles, obtained by linking OpRisk events to event types with defined exposures, in addition to "predictive" factors, through the introduction of a given set of *risk factors* who also sufficiently capture risk exposure to forward-looking aspects. As a conscequence, capital estimates adapt to real-time changes in the risk profile of a bank e.g., point-in-time changes in the portfolio mix, or the introduction of a new product. The aggregate event loss variable of the EBOR model derived from \ref{EBORmethod}, with indivudual losses yields

\singlespacing
\begin{eqnarray}\label{EBORmodel}
\mathbf{Y} =\sum_j^n I_r\cdot L_r\cdot EI_r, \quad \mbox{where}\quad r \in {i,\ldots,n}
\end{eqnarray}
\doublespacing

The EBOR model concept defines $n$ potential loss events, where $n$ is considered as the *frequency exposure* and no longer denotes $n=56$ different components of the LDA model cells corresponding to BL/ET matrix combinations, but to individual loss events. At this stage, EBOR is not conidered as an option in the Basel II accord for OpRisk. Indeed, the regulatory framework proposes four approaches i.e., SA, BIA, IMA ans the SMA.\medskip

The EBOR model is a special case of the IMA conditional on the IMA being allowed to depend only on aggregate number of events, and on the total loss amounts by BL/ET risk type cells but not on individual losses [@frachot2001loss]. @frachot2001loss demonstrates this when we find out the conditions under which both methods coincide i.e., $\mathcal{\Large{C}}_{OpRisk}^{IMA} = \mathcal{\Large{C}}_{OpRisk}^{LDA}$ and $\mbox{EL}_{k}^{IMA}=\mbox{EL}_l^{LDA}$ i.e., $Y_{ij} = S_{ij}^{T+\tau}$. It comes that the internal scaling factor $\gamma_k$:\medskip

\singlespacing
\begin{eqnarray}
\gamma_k &=& \frac{\mathcal{\Large{C}}_{OpRisk}^{IMA}}{\sum_r^n I_r\cdot L_r\cdot EI_r} = \frac{\mathcal{\Large{C}}_{OpRisk}^{LDA}}{\sum_{l=1}^{N_{k}}X_{kl}}\nonumber\\
&=& \frac{G^{-1}(\alpha)}{\sum_{l=1}^{N_{k}}X_{kl}}\\
&\Longrightarrow& \sum_r^n I_r\cdot L_r\cdot EI_r = \sum_{l=1}^{N_{k}}X_{kl}\nonumber\\
\end{eqnarray}
\doublespacing

Where $n$ is fixed to the number of observations in the internal database and $\mathbf{G}^{-1}(\alpha)$ is the quantile of $\vartheta$ for the level $\alpha$. For illustrative purposes lets assume the poisson/log-normal compound distribution where the frequency parameter ($N \sim \mathcal{P}(\theta)$), and the severity parameters ($\zeta \sim \mathcal{LN}(\mu,\sigma)$): The IMA model can justifiably be developed as a proxy for the LDA model supposedly to capture the LDA model in a simplified way provided $\gamma_k$ has the following functional form:

\singlespacing
\begin{eqnarray}\label{internalfactorcond}
\gamma = \gamma(\theta, \mu, \sigma;\alpha)
\end{eqnarray}
\doublespacing

In turn, see Section \ref{sssec:Internal measurement approach} & \ref{Internalmeasurement}, and provided expression \ref{internalfactorcond} holds, than it is justified that the EBOR model is also a special case of the LDA model [@einemann2018operational].\medskip
 
 The aggregate event loss $\mathbf{Y}$'s relates to the sum of $(I_1,\ldots,I_n)$ denoting the event indicator; a vector of independent (bernoulli) rv's, whose joint event probabilities are specified through a bernoulli mixture model defined by: $\ni \mathbb{P}(I_j=1|\Psi=\psi)=p_j(\psi)$ and $\psi=\mathbb{R}^m$, such that they have to attain values $y=(y_1,\ldots,y_n) \in \{0,1\}^n$; whose sum is called the event \textbf{frequency variable}, taking the states $1$ or $0$ depending on whether there is a realised loss, or a pending loss/near miss. $EI_j$ is the deterministic *severity exposure* of the $j_{th}$ event and $L_j$ is the (stochastic) severity ratio which specifies the loss ratio or loss-given-event (LGE) as a percentage of exposure.

\subsubsection{Integration of EBOR and LDA models}

The only missing piece for a sound Oprisk capital calculation exercise is left in merging the LDA and EBOR models in a fully integrated and diversified way [@einemann2018operational]. This setup is achieved by specifying the dependence of the LDA frequency and EBOR frequency through an additional dimension of the copula, such that the EBOR model is considered as and additional cell, anagolously to the BL/ET matrix combinations in classical LDA model.  @einemann2018operational deduced a simple recursion formula which is used for a joint LDA and EBOR simulation algorithm smoothening the EBOR modelling integration into an LDA model. The output is a total number of EBOR events, $n_{r+1}(l)$  translated into a joint state of realisations $I_1(l),\ldots,I_n(l)$ for a specific scenario, such that

\singlespacing
\begin{eqnarray}\label{EBORexposure}
n_{r+1}(l)=\sum_{j=1}^n I_j(l)
\end{eqnarray}
\doublespacing

The integration concept would also trigger changes of the LDA models input data to avoid double counting of loss potential, therefore it is assumed that the LDA and EBOR events are separated beforehand leaving the task of specifying the model.

\subsection{A new class of models capturing forward-looking aspects}
\label{sec:A new class of models capturing forward-looking aspects}

I am  using GLM's to build a "comprehensive" predictive model in the sense that including all relevant risk factors responsible for loss mechanisms can have an affect on the number and sizes of operational losses, and expand the uses of the model to a wider range of Oprisk loss types. Building the model incorporates the use of an offset feature, differentiating this modelling technique to the non-ideal actuarial model specified in @einemann2018operational. An *offset* is an additional model variable which is useful for modeling rate data.\medskip

The afore-mentioned non-ideal nature in the actuarial technique for integrating EBOR models and LDA models is compounded by the real-world fact that OpRisk data is often difficult to parse into EBOR data and LDA data types as required by the integrated model, furthermore OpRisk data is often incomplete and many relevant variables are inconsistently coded and massively categorical. For these reasons [@yan2009applications], in most actuarial modeling situations modelers are forced to exclude variables that are relevant to predicting frequency and severity of losses exacerbating the problem of OVB. In contrast, the offset option from GLM's offers it's classical uses of avoiding OVB amongst others, and is useful in predictive modelling.\medskip

In this paper, we develop a data intensive GLM analysis of the response variable viz. the loss ratio term anagolous to @einemann2018operational's frequency variable, called the LossIndicator; using an explanatory vector of $p$ random variables (rv's) $\Psi = (\psi_1,\ldots,\psi_p)$, the risk factors, which are those casual factors that create losses with random uncertainty and decidedly non-normal, and who introduce dependencies between variables including categorical outcomes and discrete counts; and an *offset* variable $d_i$ which is discussed as a measure of exposure in the context of a poisson regression. In the loss ratio modeling, the goal is to build a model targeting the response $f(y;\theta)$:= the LossIndicator, which is to be layered on to the existing plan.\medskip

The *offset* is selected as a measure of trading risk exposure: i.e., the required correction for the period in days, $d$ exposed to risk, and risk factors are the business environment and internal control factors (BEICF's) thereof e.g., information such as the trading times, trader identification, loss event capture personnel, trade status and instrument types, loss event description and reasons for the losses, loss event type categorisation, individual loss amounts, market variables which have an economic interpretation, trading desk and business line, beginning and ending date and time of the event, and settlement times, etc. 

\subsubsection{Model specification}
\label{sssec:Model specification}

As specified in the LDA model (Subsection \ref{ssec:Computing the frequency distribution}), let $\mathbf{N}_{ij}$ be the number of times of OpRisk loss event failures over time $[T,T+\tau]$. The stochastic process $N_{ij}\leq n$ is called the frequency process. $N_{ij}$ is equivalent to the r.h.s of Equation \ref{EBORexposure}, corresponding to @einemann2018operational's EBOR model *frequency exposure*, where $n$ is the maximun number of events. The unit of exposure now *n* now takes on the value for the upper bound of the rv $\mathbf{N}_{ij}$, the frequency variable in the current LDA model.  

\singlespacing
\begin{eqnarray}
\mathbf{N}_k = \sum_{k=1}^m I_k 
\end{eqnarray}
\doublespacing

Where $n$ is some terminal time $T+\tau$. @nelder1972generalized, @ohlsson2010non and @covrig2015using show that this process is a poisson process which follows a poisson distribution with parameter $\theta = \lambda$, or otherwise the rate. Here we describe the *exponential dispersion model* (EDM) of the GLM, which generates the poisson distribution by the model..

\singlespacing
\begin{eqnarray}\label{EDMpoisson}
f(y,\lambda) = \frac{\lambda^ye^{-\lambda}}{y!}
\end{eqnarray}
\doublespacing

Modeling counts as realised operartional hazard in an OpRisk group requires correction for the period $d$ exposed to risk. The exposure measue is readily incorporated into the estimation procedure and is a quantity that is roughly proportional to the risk. As this statement suggests, the offset/exposure measure must be on the same scale as the linear predictor in the basic GLM framework. So the mean frequency will be estimated by the multiplicative model [@covrig2015using \& @ohlsson2010non] corresponding to a logarithmic link function, a *log link*, where a new variable $d_i$ appears 

\singlespacing
\begin{eqnarray}\label{EQlnOffset}
\lambda_i &=& d_i\cdot e^{\beta_0}\cdot e^{\beta_1x_{i1}}\cdot e^{\beta_2x_{i2}} \ldots e^{\beta_px_{ip}}\quad \mbox{Taking logs on both sides}\nonumber\\
\mbox{ln}\lambda_i &=&  \mbox{ln}d_i + \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \ldots + \beta_px_{ip}
\end{eqnarray}
\doublespacing

where ln$d_i$ is the natural log of risk exposure, called the *offset variable* which affects the algorithm only directly before and after regression estimation, effectively replacing the rate $\lambda$ with an adjusted rate (counts divided by exposures: $R=\frac{\lambda}{d}$) as the target variable; using exposure as weight; dispensing with the offset [@yan2009applications].\medskip  

In the definition for exposure, the offset is most commonly discussed as a measure of exposure in the context of poisson regression. For example when modeling rates in some observations from an OpRisk dataset, for set entries corresponding to a $d_i=6$ month time lag between the moment the Oprisk event was conceived, $T$ until the Oprisk event is realised at $T+\tau$; while another set of events correspond to a $d_j=1$ year lag, then it is appropriate to use (log of) months of exposure as and offset. If not, model variables correlated with months of exposure might possibly pick up some of the varaition that should be explained by months of exposure, resulting in biased parameter estimates.

\subsubsection{Model illustration}
\label{sssec:Model illustration}

In their paper, the EBOR model pioneered by @einemann2018operational, a non-inflated successful claimed amount providea a plausible estimate for the capital charge for litigation risk, but mainly because it is particularly well-suited to the specific risk type dealt with i.e., due to better usage of extensive existing information [@boettrich2017recent] and the more plausible model behavior over the litigation life cycle. This is important, it fits in with the required data availability prerequisite, such as the requirement for case specific information for each litigation needed for accounting, as well as in the required identification of dependencies across the porfolio which makes it easier for outflow estimates in the provisioning process [@einemann2018operational].\medskip 

Nevertheless, their EBOR model is bound to under-perform for many other OpRisk event types whose data types fall outside of the contraints, since these EBOR models designed for litigation risk are typically designed to quantify specific aspects of OpRisk. Litigation risk have rather concentrated risk profiles i.e., litigations can be grouped into clusters whereby a court ruling for one litigation impacts the likelihood of a payment for other litigations in the same cluster and, on the other hand does not influence the the outcome of litigations outside the cluster [@einemann2018operational]. For example, in a chart of litigation settlement as a percentage of IPO issuing amounts, see @rosa2012litigation which demonstrates the highest concentrations of payments toward the end of Q2 2012 on toward early Q3, tapering off in the early and latter parts of year.\medskip

However, EBOR models are important due to wide applicability beyond capital calculation and its potential to evolve into an important tool for auditing process and early detection of potential losses.

\section{Gap in the Literature}
\label{sec:Gap in the Literature}

The existing EBOR model is specific certain kinds of "predictable" risk types, such as litigation risk, due to the concentrated nature of their risk profiles in order to capture forward-looking aspects in the OpRisk prediction problem. However, @einemann2018operational's EBOR model is not suitable for most OpRisk event types since their risk profiles are not necessarily concentrated, and also the data types don't strictly meet the data requirements for the model to work. The new EBOR model is more compelling to use since it is based on well established statistical methods, so called GLM's, whose basic ideas were introduced by @nelder1972generalized. Also when the Oprisk data set is created losses are always collected of all past individual events so not only does the loss collection data exercise (LCDE) serve for the use of a more comprehensive EBOR modelling approach, but weakening constraints regarding needed concentrated risk profiles in the afore-mentioned EBOR model is specifically dealt with and eliminated through the introduction of an offset variable - viz. *exposure* measure *d*.\medskip

The GLM's offset function for one eliminates OVB, and secondly due to the GLM's well established machine learning technique, whose efficiency is a function of the number of data points, the offset function introduces a sensitivity minimising the unscaled deviance (which is trivially equivalent to maximizing the likelihood or minimising the cost function since that deviances are intepreted as weighted sums of distances estimated means from observations) and hence estimating standard errors. The offset $d$, see Equation \ref{EQlnOffset} serves as function used to measure performance in the multiplicative poisson model over time, which is analogous to an activation in the biological neural system where some groups of neurons firing cause others to fire; commonly put "Neurons that fire together wire together."        

\section{Conclusion}
\label{sec:Conclusion}

Finance models depicting OpRisk describe human behavior, and therefore the model of uncertainty measured in the past as the best estimator for future risk, are at best subjective approximations. They are not as accurate as those in market risk and credit risk modelling which lend themselves more closely to scientific analysis. The underlying problem in estimating risk, i.e., the uncertainty due to the deviation between model prediction and what is observed in the real world, is therefore more pronounced in OpRisk modeling, compounded by the fact that, in OpRisk modelling accurate and reliable quantitative data is rarely available and often of low quality.\medskip

A comparison of statistical theories such as regression models and other related statistical techniques should be encouraged to more effectively leverage off the tools used to extract knowledge from data for prediction. Linear regression, or the slightly larger general linear model are constrained since: (i) it assumes normally distributed random errors (ii) the mean is a linear function of covariates; while GLM's work with a general class of distribution unconstrained by the normal assumption, and a monotonic transformation of the mean .viz, the offset variable, a linear function of the covariates which are capable of modeling complex functional forms, with the linear an multiplicative models as special cases[@ohlsson2010non].\medskip

The new GLM based EBOR model lays the foundation for a closer mathematical representation of the real nature of OpRisk due to a "learning" mode, that captures complexities of real OpRisk behavior through the way the GLM adapts to novel environments i.e.,   given a series of inputs the data is trained and validated and eventually produces a prediction that is as close to the actual output through "learning".\medskip

There is human element largely unawares, consisting of information that is undetectable to humans so that we are unconscious to its presence, effectively lowering the certainty in our predictions and leaving us vulnerable to cognitive biases. Over time, the learning algorithm begins to modify the individual parameters within its model so that the predicted and actual results become closer and closer, thanks to a relatively long series of trials allowing the optimization of weights linking inputs to outputs. It is through patterns in that information that we are largely unaware of that predictions could arise; or that, OpRisk management incorporates rather than dismiss the many alternatives that were not imagined, the possibility of market inefficiencies or finding value in unusual places.

\singlespacing

